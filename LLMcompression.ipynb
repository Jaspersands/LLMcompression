{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9Aty72iDyt-",
        "outputId": "e8aac81a-1a11-4f8e-e793-25aaa6d6cbb2"
      },
      "outputs": [],
      "source": [
        "%env CUBLAS_WORKSPACE_CONFIG=:16:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iCJ4BB_v8G5",
        "outputId": "d8b946b7-d9be-42da-e3d6-3bdcc485285c"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pandas tqdm samplings==0.1.7 constriction bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXbHxDv6Xgfi"
      },
      "source": [
        "# LLM-Based Compressor Implementation\n",
        "\n",
        "Below is the implementation of the LLM-based compressor.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Run the **two cells above**.\n",
        "2. Upload a file you wish to compress (ideally a `.txt` file).\n",
        "After uploading your file, update the variable at the bottom of the notebook:\n",
        "\n",
        "```python\n",
        "input_path = \"ENTER FILENAME HERE\"\n",
        "```\n",
        "3. Run the third cell to start compressing.\n",
        "4. Once it completes, two files will be created:\n",
        "   - The .bin file is the encoded version of the input.\n",
        "   - The recovered file is also written to show the decoded version of the compressed file.\n",
        "\n",
        "The compressed and recovered versions should match **exactly**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHeKhshvKY2o",
        "outputId": "8e36a180-a6b2-4870-b613-c76e7eb2b598"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LLM-based lossless compression using arithmetic coding\n",
        "\"\"\"\n",
        "\n",
        "import gzip\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import struct\n",
        "import time\n",
        "from enum import Enum\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import zstandard as zstd\n",
        "import constriction\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StaticCache\n",
        "\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-1B\"\n",
        "DTYPE = torch.float16\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "SEED = 42\n",
        "NUM_PARALLEL_SEGMENTS = 2\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "def set_determinism(seed=SEED):\n",
        "    \"\"\"\n",
        "    Configure all random number generators for reproducible results.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def normalize_pdf(logits):\n",
        "    \"\"\"\n",
        "    Convert logits to a valid probability distribution for arithmetic coding.\n",
        "    \"\"\"\n",
        "    probs = torch.softmax(logits, dim=-1).to(torch.float32)\n",
        "    vocab_size = probs.shape[-1]\n",
        "    epsilon = 1e-7\n",
        "    scale = 1.0 - (2.0 * vocab_size * epsilon)\n",
        "    return (probs * scale) + epsilon\n",
        "\n",
        "\n",
        "def format_duration(seconds):\n",
        "    \"\"\"Format a duration in seconds to a human-readable string.\"\"\"\n",
        "    if seconds < 0.01:\n",
        "        return f\"{seconds * 1000:.2f}ms\"\n",
        "    return f\"{seconds:.3f}s\"\n",
        "\n",
        "\n",
        "def get_gpu_memory_str():\n",
        "    \"\"\"Get current GPU memory usage as a formatted string.\"\"\"\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    return f\"GPU: {allocated:.1f}/{reserved:.1f} GB\"\n",
        "\n",
        "\n",
        "def report_compression(codec, duration, input_bytes, output_bytes, num_tokens=None, filename=None):\n",
        "    \"\"\"Print compression metrics to stdout.\"\"\"\n",
        "    ratio = input_bytes / output_bytes if output_bytes else float(\"inf\")\n",
        "    bits_per_byte = (output_bytes * 8) / input_bytes if input_bytes else 0\n",
        "    throughput_kb = (input_bytes / 1024) / duration if duration else 0\n",
        "\n",
        "    prefix = f\"{filename}: \" if filename else \"\"\n",
        "    metrics = (\n",
        "        f\"{prefix}{codec.value:12} | {format_duration(duration):>10} | \"\n",
        "        f\"{input_bytes} B -> {output_bytes} B | \"\n",
        "        f\"ratio: {ratio:.2f}x | {bits_per_byte:.2f} bits/byte | \"\n",
        "        f\"{throughput_kb:.1f} KB/s\"\n",
        "    )\n",
        "\n",
        "    if num_tokens:\n",
        "        tokens_per_sec = num_tokens / duration if duration else 0\n",
        "        metrics += f\" | {tokens_per_sec:.1f} tok/s\"\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "\n",
        "def report_decompression(codec, duration, input_bytes, output_bytes, num_tokens=None):\n",
        "    \"\"\"Print decompression metrics to stdout.\"\"\"\n",
        "    throughput_kb = (output_bytes / 1024) / duration if duration else 0\n",
        "\n",
        "    metrics = (\n",
        "        f\"{codec.value:12} | {format_duration(duration):>10} | \"\n",
        "        f\"{input_bytes} B -> {output_bytes} B | \"\n",
        "        f\"{throughput_kb:.1f} KB/s\"\n",
        "    )\n",
        "\n",
        "    if num_tokens:\n",
        "        tokens_per_sec = num_tokens / duration if duration else 0\n",
        "        metrics += f\" | {tokens_per_sec:.1f} tok/s\"\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "\n",
        "class Codec(Enum):\n",
        "    \"\"\"Supported compression codecs for benchmarking.\"\"\"\n",
        "\n",
        "    LLM = \"LLM\"\n",
        "    ZSTD = \"ZSTD\"\n",
        "    GZIP = \"GZIP\"\n",
        "\n",
        "\n",
        "class LLMCompressor:\n",
        "    \"\"\"\n",
        "    Compressor that uses a language model for arithmetic coding.\n",
        "\n",
        "    Uses a pretrained LLM to predict token probabilities, then encodes\n",
        "    tokens using range coding. Achieves high compression ratios on\n",
        "    natural language text by leveraging the model's predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=MODEL_NAME, max_seq_len=MAX_SEQ_LEN, dtype=DTYPE):\n",
        "        self.model_name = model_name\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dtype = dtype\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is not None:\n",
        "            return\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name, use_fast=True, trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            device_map=\"auto\",\n",
        "            attn_implementation=\"sdpa\",\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "    def create_static_cache(self, device, total_tokens, batch_size=1):\n",
        "        \"\"\"Create a static KV cache for efficient inference.\"\"\"\n",
        "        max_cache_len = min(self.max_seq_len, total_tokens)\n",
        "        return StaticCache(\n",
        "            config=self.model.config,\n",
        "            batch_size=batch_size,\n",
        "            max_cache_len=max_cache_len,\n",
        "            device=device,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "    def warmup_and_capture_graph(\n",
        "        self, device, vocab_size, past_key_values, static_input, static_position, first_tokens, batch_size=1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run warmup passes and capture the forward pass as a CUDA graph.\n",
        "        \"\"\"\n",
        "        # Warmup passes before graph capture\n",
        "        warmup_stream = torch.cuda.Stream()\n",
        "        warmup_stream.wait_stream(torch.cuda.current_stream())\n",
        "\n",
        "        # Handle single token or batch of tokens\n",
        "        if isinstance(first_tokens, int):\n",
        "            static_input.fill_(first_tokens)\n",
        "        else:\n",
        "            for i, tok in enumerate(first_tokens):\n",
        "                static_input[i, 0] = tok\n",
        "\n",
        "        with torch.cuda.stream(warmup_stream):\n",
        "            for _ in range(3):\n",
        "                self.model(\n",
        "                    input_ids=static_input,\n",
        "                    past_key_values=past_key_values,\n",
        "                    use_cache=True,\n",
        "                    cache_position=static_position.zero_(),\n",
        "                )\n",
        "\n",
        "        torch.cuda.current_stream().wait_stream(warmup_stream)\n",
        "        past_key_values.reset()\n",
        "        static_position.zero_()\n",
        "\n",
        "        # Allocate output buffer (batched or single)\n",
        "        if batch_size > 1:\n",
        "            static_probs = torch.empty((batch_size, vocab_size), device=device, dtype=torch.float32)\n",
        "        else:\n",
        "            static_probs = torch.empty((vocab_size,), device=device, dtype=torch.float32)\n",
        "\n",
        "        # Capture forward pass as CUDA graph\n",
        "        graph = torch.cuda.CUDAGraph()\n",
        "        with torch.cuda.graph(graph):\n",
        "            outputs = self.model(\n",
        "                input_ids=static_input,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                cache_position=static_position,\n",
        "            )\n",
        "            if batch_size > 1:\n",
        "                probs_tensor = normalize_pdf(outputs.logits[:, 0, :])\n",
        "            else:\n",
        "                probs_tensor = normalize_pdf(outputs.logits[0, 0, :])\n",
        "            static_probs.copy_(probs_tensor)\n",
        "\n",
        "        return graph, static_probs\n",
        "\n",
        "    def print_progress(self, current, total, segment_index, num_segments, operation):\n",
        "        \"\"\"Print progress update to stdout.\"\"\"\n",
        "        if current % 200 == 0:\n",
        "            print(\n",
        "                f\"\\r {operation} token {current}/{total} \"\n",
        "                f\"(segment {segment_index + 1}/{num_segments})\",\n",
        "                end=\"\",\n",
        "            )\n",
        "\n",
        "    def write_header(self, file_handle, total_tokens, segment_first_tokens):\n",
        "        \"\"\"\n",
        "        Write the compressed file header.\n",
        "\n",
        "        Format: total_tokens (4B) + num_segments (4B) + first_tokens (4B each)\n",
        "        \"\"\"\n",
        "        file_handle.write(struct.pack(\"<I\", total_tokens))\n",
        "        file_handle.write(struct.pack(\"<I\", len(segment_first_tokens)))\n",
        "        for token in segment_first_tokens:\n",
        "            file_handle.write(struct.pack(\"<I\", token))\n",
        "\n",
        "    def read_header(self, raw_data):\n",
        "        \"\"\"\n",
        "        Read and parse the compressed file header.\n",
        "        \"\"\"\n",
        "        if len(raw_data) < 8:\n",
        "            raise ValueError(\"File too small to contain header.\")\n",
        "\n",
        "        total_tokens = struct.unpack_from(\"<I\", raw_data, 0)[0]\n",
        "        num_segments = struct.unpack_from(\"<I\", raw_data, 4)[0]\n",
        "\n",
        "        if num_segments == 0 or total_tokens == 0:\n",
        "            raise ValueError(\"Invalid header: zero segments or tokens.\")\n",
        "\n",
        "        offset = 8\n",
        "        if len(raw_data) < offset + 4 * num_segments:\n",
        "            raise ValueError(\"File too small for segment first tokens.\")\n",
        "\n",
        "        segment_first_tokens = []\n",
        "        for _ in range(num_segments):\n",
        "            token = struct.unpack_from(\"<I\", raw_data, offset)[0]\n",
        "            segment_first_tokens.append(int(token))\n",
        "            offset += 4\n",
        "\n",
        "        return total_tokens, segment_first_tokens, offset\n",
        "\n",
        "    def run_zstd_compress_baseline(self, raw_bytes, filename=None):\n",
        "        \"\"\"Run zstd compression for baseline comparison.\"\"\"\n",
        "        start = time.time()\n",
        "        compressor = zstd.ZstdCompressor(level=22)\n",
        "        compressed = compressor.compress(raw_bytes)\n",
        "        report_compression(\n",
        "            Codec.ZSTD, time.time() - start, len(raw_bytes), len(compressed),\n",
        "            filename=filename\n",
        "        )\n",
        "\n",
        "    def run_zstd_decompress_baseline(self, raw_bytes):\n",
        "        \"\"\"Run zstd decompression for baseline comparison.\"\"\"\n",
        "        compressor = zstd.ZstdCompressor(level=22)\n",
        "        compressed = compressor.compress(raw_bytes)\n",
        "        decompressor = zstd.ZstdDecompressor()\n",
        "\n",
        "        start = time.time()\n",
        "        decompressed = decompressor.decompress(compressed)\n",
        "        report_decompression(\n",
        "            Codec.ZSTD, time.time() - start, len(compressed), len(decompressed)\n",
        "        )\n",
        "\n",
        "    def run_gzip_compress_baseline(self, raw_bytes, filename=None):\n",
        "        \"\"\"Run gzip compression for baseline comparison.\"\"\"\n",
        "        start = time.time()\n",
        "        compressed = gzip.compress(raw_bytes, compresslevel=9)\n",
        "        report_compression(\n",
        "            Codec.GZIP, time.time() - start, len(raw_bytes), len(compressed),\n",
        "            filename=filename\n",
        "        )\n",
        "\n",
        "    def run_gzip_decompress_baseline(self, raw_bytes):\n",
        "        \"\"\"Run gzip decompression for baseline comparison.\"\"\"\n",
        "        compressed = gzip.compress(raw_bytes, compresslevel=9)\n",
        "\n",
        "        start = time.time()\n",
        "        decompressed = gzip.decompress(compressed)\n",
        "        report_decompression(\n",
        "            Codec.GZIP, time.time() - start, len(compressed), len(decompressed)\n",
        "        )\n",
        "\n",
        "    def compress(self, input_path, output_path, show_baselines=True):\n",
        "        \"\"\"\n",
        "        Compress a text file using LLM-based arithmetic coding.\n",
        "        \"\"\"\n",
        "        filename = os.path.basename(input_path)\n",
        "        print(f\"\\n{'='*60}\\nCompressing: {filename}\\n{'='*60}\")\n",
        "        set_determinism()\n",
        "        self.load_model()\n",
        "\n",
        "        device = self.model.device\n",
        "        vocab_size = self.model.config.vocab_size\n",
        "\n",
        "        # Read file as UTF-8 text\n",
        "        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "\n",
        "        input_ids = self.tokenizer(content, add_special_tokens=False)[\"input_ids\"]\n",
        "        tokens = np.array(input_ids, dtype=np.int32)\n",
        "        total_tokens = len(tokens)\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            raise ValueError(\"Input has no tokens to compress.\")\n",
        "\n",
        "        num_segments = math.ceil(total_tokens / self.max_seq_len)\n",
        "        print(f\" Total tokens: {total_tokens}, segments: {num_segments}, parallel: {min(NUM_PARALLEL_SEGMENTS, num_segments)}\")\n",
        "\n",
        "        # Precompute segment info\n",
        "        segment_starts = []\n",
        "        segment_ends = []\n",
        "        segment_lengths = []\n",
        "        segment_first_tokens = []\n",
        "\n",
        "        for seg_idx in range(num_segments):\n",
        "            seg_start = seg_idx * self.max_seq_len\n",
        "            seg_end = min(seg_start + self.max_seq_len, total_tokens)\n",
        "            segment_starts.append(seg_start)\n",
        "            segment_ends.append(seg_end)\n",
        "            segment_lengths.append(seg_end - seg_start)\n",
        "            segment_first_tokens.append(int(tokens[seg_start]))\n",
        "\n",
        "        # Determine batch size (may be smaller for final batch)\n",
        "        batch_size = min(NUM_PARALLEL_SEGMENTS, num_segments)\n",
        "\n",
        "        # Setup inference state for batched processing\n",
        "        past_key_values = self.create_static_cache(device, total_tokens, batch_size=batch_size)\n",
        "        static_input = torch.empty((batch_size, 1), device=device, dtype=torch.long)\n",
        "        static_position = torch.zeros((1,), device=device, dtype=torch.long)\n",
        "\n",
        "        # Pinned CPU buffer for probability transfer\n",
        "        probs_cpu = torch.empty((batch_size, vocab_size), dtype=torch.float32, pin_memory=True)\n",
        "        probs_np = probs_cpu.numpy()\n",
        "\n",
        "        encoder = constriction.stream.queue.RangeEncoder()\n",
        "        entropy_family = constriction.stream.model.Categorical(perfect=False)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # Get first tokens for initial batch for warmup\n",
        "            first_batch_tokens = segment_first_tokens[:batch_size]\n",
        "            graph, static_probs = self.warmup_and_capture_graph(\n",
        "                device, vocab_size, past_key_values,\n",
        "                static_input, static_position, first_batch_tokens, batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            start_time = time.time()\n",
        "            num_coded_tokens = total_tokens - num_segments\n",
        "            tokens_coded = 0\n",
        "\n",
        "            # Process segments in batches\n",
        "            num_batches = math.ceil(num_segments / NUM_PARALLEL_SEGMENTS)\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                batch_start = batch_idx * NUM_PARALLEL_SEGMENTS\n",
        "                batch_end = min(batch_start + NUM_PARALLEL_SEGMENTS, num_segments)\n",
        "                active_segments = batch_end - batch_start\n",
        "\n",
        "                # Get segment info for this batch\n",
        "                batch_seg_starts = segment_starts[batch_start:batch_end]\n",
        "                batch_seg_lengths = segment_lengths[batch_start:batch_end]\n",
        "                batch_first_tokens = segment_first_tokens[batch_start:batch_end]\n",
        "                max_seg_len = max(batch_seg_lengths)\n",
        "\n",
        "                # Reset KV cache and set first tokens\n",
        "                past_key_values.reset()\n",
        "                static_position.zero_()\n",
        "\n",
        "                # Fill input with first tokens\n",
        "                for i in range(batch_size):\n",
        "                    if i < active_segments:\n",
        "                        static_input[i, 0] = batch_first_tokens[i]\n",
        "                    else:\n",
        "                        static_input[i, 0] = batch_first_tokens[0]  # Padding\n",
        "\n",
        "                for step in range(1, max_seg_len):\n",
        "                    static_position.fill_(step - 1)\n",
        "                    graph.replay()\n",
        "\n",
        "                    probs_cpu.copy_(static_probs, non_blocking=False)\n",
        "\n",
        "                    active_idx = []\n",
        "                    symbols = []\n",
        "                    for i in range(active_segments):\n",
        "                        if step < batch_seg_lengths[i]:\n",
        "                            seg_start = batch_seg_starts[i]\n",
        "                            symbols.append(int(tokens[seg_start + step]))\n",
        "                            active_idx.append(i)\n",
        "\n",
        "                    if symbols:\n",
        "                        symbols_np = np.array(symbols, dtype=np.int32)\n",
        "                        probs_active = np.ascontiguousarray(probs_np[active_idx, :])\n",
        "                        encoder.encode(symbols_np, entropy_family, probs_active)\n",
        "                        tokens_coded += len(symbols)\n",
        "\n",
        "                    for i in range(batch_size):\n",
        "                        if i < active_segments and step < batch_seg_lengths[i]:\n",
        "                            seg_start = batch_seg_starts[i]\n",
        "                            static_input[i, 0] = int(tokens[seg_start + step])\n",
        "\n",
        "                    # Progress update\n",
        "                    if tokens_coded % 200 == 0:\n",
        "                        print(\n",
        "                            f\"\\r Compressing token {tokens_coded}/{num_coded_tokens} \"\n",
        "                            f\"(batch {batch_idx + 1}/{num_batches}) | {get_gpu_memory_str()}\",\n",
        "                            end=\"\",\n",
        "                        )\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "            compressed_data = encoder.get_compressed()\n",
        "\n",
        "            with open(output_path, \"wb\") as file:\n",
        "                self.write_header(file, total_tokens, segment_first_tokens)\n",
        "                file.write(compressed_data)\n",
        "\n",
        "            input_bytes = content.encode(\"utf-8\")\n",
        "            report_compression(\n",
        "                Codec.LLM,\n",
        "                time.time() - start_time,\n",
        "                len(input_bytes),\n",
        "                os.path.getsize(output_path),\n",
        "                total_tokens,\n",
        "                filename=filename,\n",
        "            )\n",
        "\n",
        "        if show_baselines:\n",
        "            input_bytes = content.encode(\"utf-8\")\n",
        "            self.run_zstd_compress_baseline(input_bytes, filename=filename)\n",
        "            self.run_gzip_compress_baseline(input_bytes, filename=filename)\n",
        "\n",
        "    def decompress(self, input_path, output_path):\n",
        "        \"\"\"\n",
        "        Decompress a file compressed with LLM-based arithmetic coding.\n",
        "        \"\"\"\n",
        "        print(f\"Starting decompression [{input_path}]...\")\n",
        "        set_determinism()\n",
        "        self.load_model()\n",
        "\n",
        "        device = self.model.device\n",
        "        vocab_size = self.model.config.vocab_size\n",
        "\n",
        "        # Read compressed file\n",
        "        with open(input_path, \"rb\") as file:\n",
        "            raw_data = file.read()\n",
        "\n",
        "        total_tokens, segment_first_tokens, data_offset = self.read_header(raw_data)\n",
        "        num_segments = len(segment_first_tokens)\n",
        "\n",
        "        # Setup decoder\n",
        "        compressed_array = np.frombuffer(raw_data[data_offset:], dtype=np.uint32).copy()\n",
        "        decoder = constriction.stream.queue.RangeDecoder(compressed_array)\n",
        "        entropy_family = constriction.stream.model.Categorical(perfect=False)\n",
        "\n",
        "        # Precompute segment info (same as compression)\n",
        "        segment_starts = []\n",
        "        segment_lengths = []\n",
        "        for seg_idx in range(num_segments):\n",
        "            seg_start = seg_idx * self.max_seq_len\n",
        "            seg_end = min(seg_start + self.max_seq_len, total_tokens)\n",
        "            segment_starts.append(seg_start)\n",
        "            segment_lengths.append(seg_end - seg_start)\n",
        "\n",
        "        # Output array for decoded tokens\n",
        "        decoded_tokens = np.empty(total_tokens, dtype=np.int32)\n",
        "\n",
        "        # Write first tokens to their positions\n",
        "        for seg_idx, tok in enumerate(segment_first_tokens):\n",
        "            decoded_tokens[segment_starts[seg_idx]] = tok\n",
        "\n",
        "        # Batched inference setup\n",
        "        batch_size = min(NUM_PARALLEL_SEGMENTS, num_segments)\n",
        "        past_key_values = self.create_static_cache(device, total_tokens, batch_size=batch_size)\n",
        "        static_input = torch.empty((batch_size, 1), device=device, dtype=torch.long)\n",
        "        static_position = torch.zeros((1,), device=device, dtype=torch.long)\n",
        "\n",
        "        probs_cpu = torch.empty((batch_size, vocab_size), dtype=torch.float32, pin_memory=True)\n",
        "        probs_np = probs_cpu.numpy()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # Warmup with first batch tokens\n",
        "            first_batch_tokens = segment_first_tokens[:batch_size]\n",
        "            graph, static_probs = self.warmup_and_capture_graph(\n",
        "                device, vocab_size, past_key_values,\n",
        "                static_input, static_position, first_batch_tokens, batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            start_time = time.time()\n",
        "            num_coded_tokens = total_tokens - num_segments\n",
        "            tokens_decoded = 0\n",
        "\n",
        "            # Process segments in batches\n",
        "            num_batches = math.ceil(num_segments / NUM_PARALLEL_SEGMENTS)\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                batch_start = batch_idx * NUM_PARALLEL_SEGMENTS\n",
        "                batch_end = min(batch_start + NUM_PARALLEL_SEGMENTS, num_segments)\n",
        "                active_segments = batch_end - batch_start\n",
        "\n",
        "                # Get segment info for this batch\n",
        "                batch_seg_starts = segment_starts[batch_start:batch_end]\n",
        "                batch_seg_lengths = segment_lengths[batch_start:batch_end]\n",
        "                batch_first_tokens = segment_first_tokens[batch_start:batch_end]\n",
        "                max_seg_len = max(batch_seg_lengths)\n",
        "\n",
        "                # Reset KV cache and set first tokens\n",
        "                past_key_values.reset()\n",
        "                static_position.zero_()\n",
        "\n",
        "                # Fill input with first tokens\n",
        "                for i in range(batch_size):\n",
        "                    if i < active_segments:\n",
        "                        static_input[i, 0] = batch_first_tokens[i]\n",
        "                    else:\n",
        "                        static_input[i, 0] = batch_first_tokens[0]\n",
        "\n",
        "                for step in range(1, max_seg_len):\n",
        "                    static_position.fill_(step - 1)\n",
        "                    graph.replay()\n",
        "\n",
        "                    probs_cpu.copy_(static_probs, non_blocking=False)\n",
        "\n",
        "                    active_idx = []\n",
        "                    for i in range(active_segments):\n",
        "                        if step < batch_seg_lengths[i]:\n",
        "                            active_idx.append(i)\n",
        "\n",
        "                    if active_idx:\n",
        "                        probs_active = np.ascontiguousarray(probs_np[active_idx, :])\n",
        "                        decoded_step = decoder.decode(entropy_family, probs_active)\n",
        "\n",
        "                        for j, i in enumerate(active_idx):\n",
        "                            pos = batch_seg_starts[i] + step\n",
        "                            decoded_tokens[pos] = int(decoded_step[j])\n",
        "                            static_input[i, 0] = int(decoded_step[j])\n",
        "\n",
        "                        tokens_decoded += len(active_idx)\n",
        "\n",
        "                    if tokens_decoded % 200 == 0:\n",
        "                        print(\n",
        "                            f\"\\r Decompressing token {tokens_decoded}/{num_coded_tokens} \"\n",
        "                            f\"(batch {batch_idx + 1}/{num_batches}) | {get_gpu_memory_str()}\",\n",
        "                            end=\"\",\n",
        "                        )\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "            if tokens_decoded != num_coded_tokens:\n",
        "                raise RuntimeError(\n",
        "                    f\"Token count mismatch: expected {num_coded_tokens}, \"\n",
        "                    f\"decoded {tokens_decoded}\"\n",
        "                )\n",
        "\n",
        "            text = self.tokenizer.decode(\n",
        "                decoded_tokens.tolist(), clean_up_tokenization_spaces=False\n",
        "            )\n",
        "\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(text)\n",
        "\n",
        "            output_bytes = text.encode(\"utf-8\")\n",
        "            report_decompression(\n",
        "                Codec.LLM,\n",
        "                time.time() - start_time,\n",
        "                len(raw_data),\n",
        "                len(output_bytes),\n",
        "                total_tokens,\n",
        "            )\n",
        "\n",
        "        self.run_zstd_decompress_baseline(output_bytes)\n",
        "        self.run_gzip_decompress_baseline(output_bytes)\n",
        "\n",
        "\n",
        "    def compress_directory(self, input_dir):\n",
        "        \"\"\"\n",
        "        Compress all files in a directory.\n",
        "\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(input_dir):\n",
        "            raise ValueError(f\"Not a directory: {input_dir}\")\n",
        "\n",
        "        output_dir = os.path.join(input_dir, \"compressed\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        files = []\n",
        "        for filename in sorted(os.listdir(input_dir)):\n",
        "            filepath = os.path.join(input_dir, filename)\n",
        "            if os.path.isfile(filepath) and not filename.endswith(\".bin\"):\n",
        "                files.append(filepath)\n",
        "\n",
        "        if not files:\n",
        "            print(f\"No files found in {input_dir}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(files)} files to compress\")\n",
        "\n",
        "        self.load_model()\n",
        "\n",
        "        for filepath in files:\n",
        "            filename = os.path.basename(filepath)\n",
        "            output_path = os.path.join(output_dir, filename + \".bin\")\n",
        "\n",
        "            try:\n",
        "                self.compress(filepath, output_path, show_baselines=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error compressing {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Compression complete. Output directory: {output_dir}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "    def decompress_directory(self, input_dir):\n",
        "        if not os.path.isdir(input_dir):\n",
        "            raise ValueError(f\"Not a directory: {input_dir}\")\n",
        "\n",
        "        output_dir = os.path.join(input_dir, \"recovered\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Find all .bin files\n",
        "        files = []\n",
        "        for filename in sorted(os.listdir(input_dir)):\n",
        "            filepath = os.path.join(input_dir, filename)\n",
        "            if os.path.isfile(filepath) and filename.endswith(\".bin\"):\n",
        "                files.append(filepath)\n",
        "\n",
        "        if not files:\n",
        "            print(f\"No .bin files found in {input_dir}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(files)} files to decompress\")\n",
        "\n",
        "        self.load_model()\n",
        "\n",
        "        for filepath in files:\n",
        "            filename = os.path.basename(filepath)\n",
        "            original_name = filename[:-4] if filename.endswith(\".bin\") else filename\n",
        "            output_path = os.path.join(output_dir, original_name)\n",
        "\n",
        "            try:\n",
        "                self.decompress(filepath, output_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error decompressing {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Decompression complete. Output directory: {output_dir}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_path = \"ENTER FILENAME HERE\" # For example, input_path = \"test.txt\"\n",
        "    compressor = LLMCompressor()\n",
        "\n",
        "    if os.path.isdir(input_path):\n",
        "        # Compress all files, then decompress all, then verify\n",
        "        compressor.compress_directory(input_path)\n",
        "        compressed_dir = os.path.join(input_path, \"compressed\")\n",
        "        compressor.decompress_directory(compressed_dir)\n",
        "\n",
        "        # Verify all files\n",
        "        recovered_dir = os.path.join(compressed_dir, \"recovered\")\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Verifying integrity...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        for filename in sorted(os.listdir(input_path)):\n",
        "            original_path = os.path.join(input_path, filename)\n",
        "            if os.path.isfile(original_path) and not filename.endswith(\".bin\"):\n",
        "                recovered_path = os.path.join(recovered_dir, filename)\n",
        "                if os.path.exists(recovered_path):\n",
        "                    pass\n",
        "\n",
        "    elif os.path.isfile(input_path):\n",
        "        compressed_path = input_path + \".bin\"\n",
        "        recovered_path = input_path + \".recovered\"\n",
        "\n",
        "        compressor.compress(input_path, compressed_path)\n",
        "        compressor.decompress(compressed_path, recovered_path)\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Path not found: {input_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
